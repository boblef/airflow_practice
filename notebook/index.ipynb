{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework2 outlines\n",
    "![airflow-logo](https://user-images.githubusercontent.com/44624585/61552718-6fe3af00-aa0d-11e9-8563-45253ed5e4c6.jpeg)\n",
    "\n",
    "#### Task:\n",
    "Create a data pipeline using Airflow that perform ETL jobs to extract data and put into a storage of your choice.\n",
    "\n",
    "#### What I created\n",
    "1. Scrape headlines of top news from [CBC](https://www.cbc.ca/news/canada)\n",
    "2. Store them into a csv file with.\n",
    "\n",
    "#### Table of contents\n",
    "- Talk about more details of what I created.\n",
    "    - Scraping\n",
    "    - Writing CSV file\n",
    "- Look at the code.\n",
    "- How to use.\n",
    "- Import csv to Pandas Dataframe\n",
    "- What can we do with this\n",
    "- Resources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talk about more details of what I created.\n",
    "\n",
    "### Scraping\n",
    "First of all, what it does is scrape the headlines of top news on [CBC](https://www.cbc.ca/news/canada) using [BeautifulSoap](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), which is a Python library for web scraping.<br>\n",
    "This is the top page of CBC at that time I am creating this Notebook, let's have a look at the headline of the left news where you can see Japanese flag. \n",
    "<img width=\"1435\" alt=\"ss1\" src=\"https://user-images.githubusercontent.com/44624585/61552759-8e49aa80-aa0d-11e9-92a6-2c1518fd4a56.png\">\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "This is an example of headlines.<br>\n",
    "<img width=\"567\" alt=\"ss2\" src=\"https://user-images.githubusercontent.com/44624585/61552804-aa4d4c00-aa0d-11e9-8553-1397cce4b7df.png\"><br>\n",
    "**This is a headline I am going to get.** On this website, every headline of news is inside of `<h3>` tag.\n",
    "\n",
    "So we first scrape all of the text inside of `<h3>` tags on the top page but we need pay attention to things scraped.\n",
    "```\n",
    "avoid_words = [\"My Local\",\n",
    "              \"Editor's Blog\",\n",
    "              \"Connect with CBC\",\n",
    "              \"Contact CBC\",\n",
    "              \"Services & Info\",\n",
    "              \"Accessibility\"]\n",
    "```\n",
    "I created this list that contains words that the python script scrapes because those are also inside of `<h3>` tags but we don't want to write them into csv file.\n",
    "In the picture below, you can find some of the words such as `Connect with CBC`, `Contact CBC`, etc.\n",
    "<img width=\"1433\" alt=\"ss3\" src=\"https://user-images.githubusercontent.com/44624585/61552970-1465f100-aa0e-11e9-97a6-ae5deecbfb51.png\">\n",
    "<br>\n",
    "<br>\n",
    "After scraping the headlines, it creates a 2 dimensional list that contains information below.\n",
    "1. Headlines which we just scraped\n",
    "2. Source url: where did we scrape from. (https://www.cbc.ca/news/canada)\n",
    "3. Datetime\n",
    "\n",
    "The reason why we create 2 dimensional list instead of 1 dimension is that it is convenient when we write them into csv file.\n",
    "\n",
    "This list below is an example:<br>\n",
    "```\n",
    "list_example = [[\"Headline1\", \"https://www.cbc.ca/news/canada\", \"2019-07-16 15:29:00\"],\n",
    "              [\"Headline2\", \"https://www.cbc.ca/news/canada\", \"2019-07-16 15:29:00\"],\n",
    "              :\n",
    "              :\n",
    "              :\n",
    "              [\"HeadlineN\", \"https://www.cbc.ca/news/canada\", \"2019-07-16\" 15:29:00]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_from_cbc(url):\n",
    "    \"\"\"\n",
    "    Scrape headlines of news on https://www.cbc.ca/news/canada.\n",
    "    Then write them into a csv file with additional information\n",
    "    such as time and url.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: str\n",
    "    A URL to a website where I want to scrape from\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    current_top_news: list\n",
    "    This is a list, and each element is also a list that contains\n",
    "    1. headline\n",
    "    2. source url\n",
    "    3. date and time\n",
    "\n",
    "    Example:\n",
    "    [[\"headline1\", \"url\", datetime],\n",
    "     [\"headline2\", \"url\", datetime]]\n",
    "\n",
    "    Where \"headline\" and \"url\" are string, datetime is datetime object.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.encoding = response.apparent_encoding\n",
    "\n",
    "    bs = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    datetime_now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    current_top_news = []\n",
    "    for i in bs.select(\"h3\"):\n",
    "        if not i.getText() in avoid_words:\n",
    "            print(i.getText())  # Print each headline.\n",
    "            current_top_news.append(list((i.getText(), url, datetime_now)))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return current_top_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing CSV file\n",
    "Now we have a list that some headlines of news on CBC. The next things we will do is to write them into csv file. So I created a function but **how do we pass variables to functions in Airflow**.\n",
    "In order to pass variables to a function in Airflow, we need to use [Xcoms](https://airflow.apache.org/concepts.html#xcoms).\n",
    "\n",
    "#### Xcoms\n",
    "XComs, or short for \"cross communication\" are stores of key, value, and timestamps meant to communicate between tasks. XComs are stored in Airflow's metadata database with an associated execution_date, TaskInstance and DagRun.\n",
    "\n",
    "XComs can be `\"pushed\"` or `\"pulled\"` by all TaskInstances (by using `xcom_push()` or `xcom_pull()`, respectively).\n",
    "\n",
    "When we use `return` in a function, it automatically does `push` so that I didn't use `xcom_push()` in this script. But inside of `write_csv()` function, which I created, I used `xcom_pull()` to get the list which we just created and which has the headlines, urls, and datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(**kwargs):\n",
    "    \"\"\"\n",
    "    Write information in the given list into a csv file.\n",
    "    To get the list we just created, we use Xcoms.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Boolean: True or False\n",
    "\n",
    "    When we success writing things correctly, returns True. Otherwise False.\n",
    "    \"\"\"\n",
    "\n",
    "    # Xcoms to get the list\n",
    "    ti = kwargs['ti']\n",
    "    current_top_news = ti.xcom_pull(task_ids='scraping')\n",
    "\n",
    "    try:\n",
    "        with open(str(csv_path), \"a\") as file:\n",
    "            writer = csv.writer(file, lineterminator='\\n')\n",
    "            writer.writerows(current_top_news)\n",
    "        return True\n",
    "    except OSError as e:\n",
    "        print(e)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the code\n",
    "First of all, I will provide you the whole code, and basically, you can use this code by just copy and paste.\n",
    "But we will take a look at each code later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "# Define cvs file path\n",
    "csv_path = Path(\"../data/index.csv\")\n",
    "\n",
    "# List of words that script should avoid to write onto csv.\n",
    "avoid_words = [\"My Local\",\n",
    "               \"Editor's Blog\",\n",
    "               \"Connect with CBC\",\n",
    "               \"Contact CBC\",\n",
    "               \"Services & Info\",\n",
    "               \"Accessibility\"]\n",
    "\n",
    "# Define URL where scrape from.\n",
    "cbc_url = \"https://www.cbc.ca/news/canada\"\n",
    "\n",
    "# Default args used when create a new dag\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2019, 7, 18),\n",
    "    # 'end_date': datetime(2018, 12, 30),\n",
    "    'depends_on_past': False,\n",
    "    'email': ['kohei.suzuki808@gmail.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    # If a task fails, retry it once after waiting\n",
    "    # at least 5 minutes\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'schedule_interval': '@once',\n",
    "}\n",
    "\n",
    "\n",
    "# Create a new dag\n",
    "dag = DAG(\n",
    "    'hw2',\n",
    "    default_args=default_args,\n",
    "    description='Homework2',\n",
    "    # Continue to run DAG once per day\n",
    "    schedule_interval=timedelta(days=1),\n",
    ")\n",
    "\n",
    "\n",
    "# Define task1\n",
    "task1 = BashOperator(\n",
    "    task_id=\"echo1\",\n",
    "    bash_command=\"echo Start scraping.\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "\n",
    "def get_news_from_cbc(url):\n",
    "    \"\"\"\n",
    "    Scrape headlines of news on https://www.cbc.ca/news/canada.\n",
    "    Then write them into a csv file with additional information\n",
    "    such as time and url.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: str\n",
    "    A URL to a website where I want to scrape from\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    current_top_news: list\n",
    "    This is a list, and each element is also a list that contains\n",
    "    1. headline\n",
    "    2. source url\n",
    "    3. date and time\n",
    "\n",
    "    Example:\n",
    "    [[\"headline1\", \"url\", datetime],\n",
    "     [\"headline2\", \"url\", datetime]]\n",
    "\n",
    "    Where \"headline\" and \"url\" are string, datetime is datetime object.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.encoding = response.apparent_encoding\n",
    "\n",
    "    bs = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    datetime_now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    current_top_news = []\n",
    "    for i in bs.select(\"h3\"):\n",
    "        if not i.getText() in avoid_words:\n",
    "            print(i.getText())  # Print each headline.\n",
    "            current_top_news.append(list((i.getText(), url, datetime_now)))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return current_top_news\n",
    "\n",
    "\n",
    "# Define task2\n",
    "task2 = PythonOperator(\n",
    "    task_id=\"scraping\",\n",
    "    python_callable=get_news_from_cbc,\n",
    "    op_kwargs={'url': cbc_url},\n",
    "    dag=dag)\n",
    "\n",
    "\n",
    "def write_csv(**kwargs):\n",
    "    \"\"\"\n",
    "    Write information in the given list into a csv file.\n",
    "    To get the list we just created, we use Xcoms.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Boolean: True or False\n",
    "\n",
    "    When we success writing things correctly, returns True. Otherwise False.\n",
    "    \"\"\"\n",
    "\n",
    "    # Xcoms to get the list\n",
    "    ti = kwargs['ti']\n",
    "    current_top_news = ti.xcom_pull(task_ids='scraping')\n",
    "\n",
    "    try:\n",
    "        with open(str(csv_path), \"a\") as file:\n",
    "            writer = csv.writer(file, lineterminator='\\n')\n",
    "            writer.writerows(current_top_news)\n",
    "        return True\n",
    "    except OSError as e:\n",
    "        print(e)\n",
    "        return False\n",
    "\n",
    "\n",
    "task3 = PythonOperator(\n",
    "    task_id=\"writing_csv\",\n",
    "    python_callable=write_csv,\n",
    "    provide_context=True,\n",
    "    dag=dag)\n",
    "\n",
    "\n",
    "def confirmation(**kwargs):\n",
    "    \"\"\"\n",
    "    If everything is done properly, print \"Done!!!!!!\"\n",
    "    Otherwise print \"Failed.\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Xcoms to get status which is the return value of write_csv().\n",
    "    ti = kwargs['ti']\n",
    "    status = ti.xcom_pull(task_ids='writing_csv')\n",
    "\n",
    "    if status:\n",
    "        print(\"Done!!!!!!\")\n",
    "    else:\n",
    "        print(\"Failed.\")\n",
    "\n",
    "\n",
    "task4 = PythonOperator(\n",
    "    task_id=\"confirmation\",\n",
    "    python_callable=confirmation,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "task1 >> task2 >> task3 >> task4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use\n",
    "1. Create a new python file by copy and past the above whole code.\n",
    "2. Put the file into your `dags` folder.\n",
    "3. Run `airflow scheduler` which will add the new dag to your dags list.\n",
    "\n",
    "#### If you want to run it in Terminal\n",
    "1. check the dags list: `airflow list_dags`\n",
    "2. Run `airflow test hw2 echo1 2019-07-16`, where will run `echo1` task.\n",
    "3. Run `airflow test hw2 scraping 2019-07-16`, where will run `scraping` task.\n",
    "4. Run `airflow test hw2 writing_csv 2019-07-16`, where will run `writing_csv` task.\n",
    "5. Run `airflow test hw2 confirmation 2019-07-16`, where will run `confirmation` task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import csv to Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path(\"../data/index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"head_lines\", \"source\", \"date\"]\n",
    "df = pd.read_csv(str(csv_path), names=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head_lines</th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ottawa stands by U.S. as safe third country, d...</td>\n",
       "      <td>https://www.cbc.ca/news/canada</td>\n",
       "      <td>2019-07-18 15:36:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toronto mayor wants independent review after m...</td>\n",
       "      <td>https://www.cbc.ca/news/canada</td>\n",
       "      <td>2019-07-18 15:36:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ottawa to pay nearly $1B to settle sexual misc...</td>\n",
       "      <td>https://www.cbc.ca/news/canada</td>\n",
       "      <td>2019-07-18 15:36:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12,000 L of oil spilled into ocean off Newfoun...</td>\n",
       "      <td>https://www.cbc.ca/news/canada</td>\n",
       "      <td>2019-07-18 15:36:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Exemplary' medical student who raped unconsci...</td>\n",
       "      <td>https://www.cbc.ca/news/canada</td>\n",
       "      <td>2019-07-18 15:36:52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          head_lines  \\\n",
       "0  Ottawa stands by U.S. as safe third country, d...   \n",
       "1  Toronto mayor wants independent review after m...   \n",
       "2  Ottawa to pay nearly $1B to settle sexual misc...   \n",
       "3  12,000 L of oil spilled into ocean off Newfoun...   \n",
       "4  'Exemplary' medical student who raped unconsci...   \n",
       "\n",
       "                           source                 date  \n",
       "0  https://www.cbc.ca/news/canada  2019-07-18 15:36:52  \n",
       "1  https://www.cbc.ca/news/canada  2019-07-18 15:36:52  \n",
       "2  https://www.cbc.ca/news/canada  2019-07-18 15:36:52  \n",
       "3  https://www.cbc.ca/news/canada  2019-07-18 15:36:52  \n",
       "4  https://www.cbc.ca/news/canada  2019-07-18 15:36:52  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we do with this\n",
    "We can apply a text classification model to classify headlines to categories such as `sport`, `economics`, etc, and create a new `category` column to keep classified categories so that we can use the data for different purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- [Airflow tutorial](https://airflow.apache.org/tutorial.html)\n",
    "- [CBC](https://www.cbc.ca/news/canada)\n",
    "- [Using Airflow Datastores](https://www.astronomer.io/guides/airflow-datastores/)\n",
    "- [Python Airflow - Return result from PythonOperator](https://stackoverflow.com/questions/50149085/python-airflow-return-result-from-pythonoperator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
